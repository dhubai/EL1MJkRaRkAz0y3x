{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe75be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197c3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Google Sheets link\n",
    "data_url = \"https://docs.google.com/spreadsheets/d/117X6i53dKiO7w6kuA1g1TpdTlv1173h_dPlJt5cNNMU/export?format=csv\"\n",
    "dataset_original = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11a9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting & Cleaning the dataset\n",
    "dataset_cleaned_temp = dataset_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a522ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id              0\n",
      "job_title       0\n",
      "location        0\n",
      "connection      0\n",
      "fit           104\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking missing data\n",
    "print(dataset_cleaned_temp.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0bb68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Checking duplicates\n",
    "print(dataset_cleaned_temp.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383e6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary words & Replace abbreviations\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = PorterStemmer()\n",
    "abbreviations_to_replace = {\n",
    "    'GPHR': 'Global Professional in Human Resources',\n",
    "    'CSR': 'Corporate Social Responsibility',\n",
    "    'MES': 'Manufacturing Execution Systems',\n",
    "    'SPHR': 'Senior Professional in Human Resources',\n",
    "    'SVP': 'Senior Vice President',\n",
    "    'GIS': 'Geographic Information System',\n",
    "    'RRP': 'Reduced Risk Products',\n",
    "    'CHRO': 'Chief Human Resources Officer',\n",
    "    'HRIS': 'Human resources information system',\n",
    "    'HR': 'Human resources',\n",
    "}\n",
    "\n",
    "def replace_abbreviations(sentence):\n",
    "    replaced_sentence = sentence\n",
    "    for abbreviation, replacement in abbreviations_to_replace.items():\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(abbreviation))\n",
    "        replaced_sentence = re.sub(pattern, replacement, replaced_sentence, flags=re.IGNORECASE)\n",
    "    return replaced_sentence\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    new_sentence = re.sub(r'[+*,.|(){}&\\-\\']', '', sentence)\n",
    "    new_sentence = replace_abbreviations(new_sentence)\n",
    "    words = new_sentence.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    lemmatized_sentence = \" \".join([token.lemma_ for token in spacy_nlp(\" \".join(stemmed_words)) if not token.is_stop])\n",
    "    return lemmatized_sentence\n",
    "\n",
    "dataset_cleaned_temp['job_title_cleaned'] = dataset_cleaned_temp['job_title'].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b2036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "dataset_preprocessed = dataset_cleaned_temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ddc62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Setup BERT & Utils\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_bert_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        encoded_inputs = bert_tokenizer(sentence, padding=True, truncation=True, return_tensors='tf')\n",
    "        outputs = bert_model(encoded_inputs)\n",
    "        embeddings.append(tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy().reshape(-1))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e99c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=clean_sentence(job_title).split(), tags=[str(i)]) for i, job_title in enumerate(dataset_preprocessed['job_title_cleaned'])]\n",
    "doc2vec_model = Doc2Vec(vector_size=768, window=2, min_count=1, workers=4, epochs=40)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "def get_doc2vec_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        embeddings.append(doc2vec_model.infer_vector(sentence.split()))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b51dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and get similarity\n",
    "def encode_and_get_similarity(data, queries, search_columns, output_columns):\n",
    "    data = data.copy()\n",
    "    bert_embeddings = {}\n",
    "    doc2vec_embeddings = {}\n",
    "    queries_embeddings = []\n",
    "    doc2vec_queries_embeddings = []\n",
    "\n",
    "    for index, query in enumerate(queries):\n",
    "        query_cleaned = replace_abbreviations(query)\n",
    "        query_cleaned = clean_sentence(query_cleaned)\n",
    "        queries_embeddings.append(get_bert_embeddings([query_cleaned]))\n",
    "        doc2vec_queries_embeddings.append(get_doc2vec_embeddings([query_cleaned]))\n",
    "\n",
    "    queries_embeddings_mean = np.mean(queries_embeddings, axis=0)\n",
    "    doc2vec_queries_embeddings_mean = np.mean(doc2vec_queries_embeddings, axis=0)\n",
    "\n",
    "    for index, column in enumerate(search_columns):\n",
    "        sentences = dataset_preprocessed[column].tolist()\n",
    "        bert_embeddings[column] = get_bert_embeddings(sentences)\n",
    "        doc2vec_embeddings[column] = get_doc2vec_embeddings(sentences)\n",
    "        \n",
    "        bert_cosine_similarities = cosine_similarity(queries_embeddings_mean, bert_embeddings[column])\n",
    "        doc2vec_cosine_similarities = cosine_similarity(doc2vec_queries_embeddings_mean, doc2vec_embeddings[column])\n",
    "        \n",
    "        data[output_columns[0]] = bert_cosine_similarities[0]\n",
    "        data[output_columns[1]] = doc2vec_cosine_similarities[0]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40f7194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search queries/keywords\n",
    "queries = [\n",
    "    'aspiring human resources',\n",
    "    'seeking human resources'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a9fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings & similarities\n",
    "dataset_preprocessed = encode_and_get_similarity(dataset_preprocessed, queries, ['job_title_cleaned'], ['bert_similarity', 'doc2vec_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c30fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of BERT and Doc2Vec similarities\n",
    "dataset_preprocessed['mean_score'] = dataset_preprocessed[['bert_similarity', 'doc2vec_similarity']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9450d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe based on the new mean_score in descending order\n",
    "dataset_preprocessed = dataset_preprocessed.sort_values(by='mean_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eadf930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Rank for BERT and Doc2Vec using mean_score\n",
    "first_rank_bert = dataset_preprocessed.sort_values(by='bert_similarity', ascending=False).head(20)\n",
    "first_rank_doc2vec = dataset_preprocessed.sort_values(by='doc2vec_similarity', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8799abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the ids of the candidates you want to star (separate by spaces): \n"
     ]
    }
   ],
   "source": [
    "# Starred Candidates\n",
    "# Mark them as favorite/bookmark\n",
    "starred_ids = [int(item) for item in input(\"Enter the ids of the candidates you want to star (separate by spaces): \").split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78325519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSD\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\MSD\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Second Rank (Re-Rank)\n",
    "dataset_preprocessed.loc[dataset_preprocessed['id'].isin(starred_ids), 'is_starred'] = 1\n",
    "dataset_preprocessed.loc[~dataset_preprocessed['id'].isin(starred_ids), 'is_starred'] = 0\n",
    "\n",
    "def get_starred_score(data):\n",
    "    data = data.copy()\n",
    "    queries = data[data['is_starred'] == 1]['job_title_cleaned']\n",
    "    similarities = []\n",
    "    for query in queries:\n",
    "        print('START: ' + query)\n",
    "        data = encode_and_get_similarity(data, [query], ['job_title_cleaned'], ['starred_similarity'])\n",
    "        similarities.append(data['starred_similarity'])\n",
    "        \n",
    "    starred_similarity = np.mean(similarities, axis=0)\n",
    "    return starred_similarity\n",
    "\n",
    "dataset_preprocessed['starred_similarity'] = get_starred_score(dataset_preprocessed)\n",
    "dataset_preprocessed['mean_similarity_bert'] = dataset_preprocessed[['bert_similarity', 'starred_similarity']].mean(axis=1)\n",
    "dataset_preprocessed['mean_similarity_doc2vec'] = dataset_preprocessed[['doc2vec_similarity', 'starred_similarity']].mean(axis=1)\n",
    "\n",
    "final_rank_bert = dataset_preprocessed[['job_title', 'is_starred', 'bert_similarity', 'starred_similarity', 'mean_similarity_bert']].sort_values(by=['mean_similarity_bert', 'is_starred'], ascending=False).head(20)\n",
    "final_rank_doc2vec = dataset_preprocessed[['job_title', 'is_starred', 'doc2vec_similarity', 'starred_similarity', 'mean_similarity_doc2vec']].sort_values(by=['mean_similarity_doc2vec', 'is_starred'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dde042f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Rank using BERT embeddings:\n",
      "                                             job_title  is_starred  \\\n",
      "59                 Aspiring Human Resources Specialist         0.0   \n",
      "35                 Aspiring Human Resources Specialist         0.0   \n",
      "5                  Aspiring Human Resources Specialist         0.0   \n",
      "48                 Aspiring Human Resources Specialist         0.0   \n",
      "23                 Aspiring Human Resources Specialist         0.0   \n",
      "98                    Seeking Human Resources Position         0.0   \n",
      "67             Human Resources Specialist at Luxottica         0.0   \n",
      "87                    Human Resources Management Major         0.0   \n",
      "100              Human Resources Generalist at Loparex         0.0   \n",
      "96               Aspiring Human Resources Professional         0.0   \n",
      "45               Aspiring Human Resources Professional         0.0   \n",
      "16               Aspiring Human Resources Professional         0.0   \n",
      "57               Aspiring Human Resources Professional         0.0   \n",
      "20               Aspiring Human Resources Professional         0.0   \n",
      "32               Aspiring Human Resources Professional         0.0   \n",
      "2                Aspiring Human Resources Professional         0.0   \n",
      "88                     Director Human Resources  at EY         0.0   \n",
      "81   Aspiring Human Resources Professional | An ene...         0.0   \n",
      "50                                HR Senior Specialist         0.0   \n",
      "7                                 HR Senior Specialist         0.0   \n",
      "\n",
      "     bert_similarity  starred_similarity  mean_similarity_bert  \n",
      "59          0.918639                 NaN              0.918639  \n",
      "35          0.918639                 NaN              0.918639  \n",
      "5           0.918639                 NaN              0.918639  \n",
      "48          0.918639                 NaN              0.918639  \n",
      "23          0.918639                 NaN              0.918639  \n",
      "98          0.899539                 NaN              0.899539  \n",
      "67          0.890844                 NaN              0.890844  \n",
      "87          0.883634                 NaN              0.883634  \n",
      "100         0.867990                 NaN              0.867990  \n",
      "96          0.860986                 NaN              0.860986  \n",
      "45          0.860986                 NaN              0.860986  \n",
      "16          0.860986                 NaN              0.860986  \n",
      "57          0.860986                 NaN              0.860986  \n",
      "20          0.860986                 NaN              0.860986  \n",
      "32          0.860986                 NaN              0.860986  \n",
      "2           0.860986                 NaN              0.860986  \n",
      "88          0.845359                 NaN              0.845359  \n",
      "81          0.839728                 NaN              0.839728  \n",
      "50          0.837753                 NaN              0.837753  \n",
      "7           0.837753                 NaN              0.837753  \n",
      "Final Rank using Doc2Vec embeddings:\n",
      "                                             job_title  is_starred  \\\n",
      "74   Nortia Staffing is seeking Human Resources, Pa...         0.0   \n",
      "103   Director Of Administration at Excellence Logging         0.0   \n",
      "99   Aspiring Human Resources Manager | Graduating ...         0.0   \n",
      "92   Admissions Representative at Community medical...         0.0   \n",
      "76   Human Resources|\\nConflict Management|\\nPolici...         0.0   \n",
      "66   Human Resources, Staffing and Recruiting Profe...         0.0   \n",
      "69   Retired Army National Guard Recruiter, office ...         0.0   \n",
      "68   Director of Human Resources North America, Gro...         0.0   \n",
      "84   RRP Brand Portfolio Executive at JTI (Japan To...         0.0   \n",
      "86   Bachelor of Science in Biology from Victoria U...         0.0   \n",
      "75   Aspiring Human Resources Professional | Passio...         0.0   \n",
      "30   2019 C.T. Bauer College of Business Graduate (...         0.0   \n",
      "63   SVP, CHRO, Marketing & Communications, CSR Off...         0.0   \n",
      "79            Junior MES Engineer| Information Systems         0.0   \n",
      "0    2019 C.T. Bauer College of Business Graduate (...         0.0   \n",
      "41   SVP, CHRO, Marketing & Communications, CSR Off...         0.0   \n",
      "78   Liberal Arts Major. Aspiring Human Resources A...         0.0   \n",
      "54   SVP, CHRO, Marketing & Communications, CSR Off...         0.0   \n",
      "90        Lead Official at Western Illinois University         0.0   \n",
      "43   2019 C.T. Bauer College of Business Graduate (...         0.0   \n",
      "\n",
      "     doc2vec_similarity  starred_similarity  mean_similarity_doc2vec  \n",
      "74             0.973763                 NaN                 0.973763  \n",
      "103            0.973684                 NaN                 0.973684  \n",
      "99             0.973660                 NaN                 0.973660  \n",
      "92             0.973651                 NaN                 0.973651  \n",
      "76             0.973649                 NaN                 0.973649  \n",
      "66             0.973630                 NaN                 0.973630  \n",
      "69             0.973611                 NaN                 0.973611  \n",
      "68             0.973600                 NaN                 0.973600  \n",
      "84             0.973579                 NaN                 0.973579  \n",
      "86             0.973579                 NaN                 0.973579  \n",
      "75             0.973574                 NaN                 0.973574  \n",
      "30             0.973563                 NaN                 0.973563  \n",
      "63             0.973557                 NaN                 0.973557  \n",
      "79             0.973546                 NaN                 0.973546  \n",
      "0              0.973543                 NaN                 0.973543  \n",
      "41             0.973541                 NaN                 0.973541  \n",
      "78             0.973539                 NaN                 0.973539  \n",
      "54             0.973535                 NaN                 0.973535  \n",
      "90             0.973532                 NaN                 0.973532  \n",
      "43             0.973518                 NaN                 0.973518  \n"
     ]
    }
   ],
   "source": [
    "print(\"Final Rank using BERT embeddings:\")\n",
    "print(final_rank_bert)\n",
    "\n",
    "print(\"Final Rank using Doc2Vec embeddings:\")\n",
    "print(final_rank_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41023787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top candidates sorted by mean similarity score:\n",
      "                                             job_title  bert_similarity  \\\n",
      "59                 Aspiring Human Resources Specialist         0.918639   \n",
      "35                 Aspiring Human Resources Specialist         0.918639   \n",
      "5                  Aspiring Human Resources Specialist         0.918639   \n",
      "98                    Seeking Human Resources Position         0.899539   \n",
      "67             Human Resources Specialist at Luxottica         0.890844   \n",
      "87                    Human Resources Management Major         0.883634   \n",
      "48                 Aspiring Human Resources Specialist         0.918639   \n",
      "23                 Aspiring Human Resources Specialist         0.918639   \n",
      "100              Human Resources Generalist at Loparex         0.867990   \n",
      "96               Aspiring Human Resources Professional         0.860986   \n",
      "45               Aspiring Human Resources Professional         0.860986   \n",
      "16               Aspiring Human Resources Professional         0.860986   \n",
      "57               Aspiring Human Resources Professional         0.860986   \n",
      "20               Aspiring Human Resources Professional         0.860986   \n",
      "32               Aspiring Human Resources Professional         0.860986   \n",
      "2                Aspiring Human Resources Professional         0.860986   \n",
      "88                     Director Human Resources  at EY         0.845359   \n",
      "81   Aspiring Human Resources Professional | An ene...         0.839728   \n",
      "80   Senior Human Resources Business Partner at Hei...         0.836062   \n",
      "78   Liberal Arts Major. Aspiring Human Resources A...         0.833361   \n",
      "\n",
      "     doc2vec_similarity  mean_score  \n",
      "59             0.962629    0.940634  \n",
      "35             0.962325    0.940482  \n",
      "5              0.956887    0.937763  \n",
      "98             0.967334    0.933436  \n",
      "67             0.971823    0.931333  \n",
      "87             0.972749    0.928191  \n",
      "48             0.936688    0.927663  \n",
      "23             0.925236    0.921937  \n",
      "100            0.970543    0.919267  \n",
      "96             0.972159    0.916573  \n",
      "45             0.971713    0.916349  \n",
      "16             0.971474    0.916230  \n",
      "57             0.971371    0.916179  \n",
      "20             0.971140    0.916063  \n",
      "32             0.971013    0.916000  \n",
      "2              0.970711    0.915849  \n",
      "88             0.973256    0.909308  \n",
      "81             0.973269    0.906499  \n",
      "80             0.973488    0.904775  \n",
      "78             0.973539    0.903450  \n"
     ]
    }
   ],
   "source": [
    "print(\"Top candidates sorted by mean similarity score:\")\n",
    "print(dataset_preprocessed[['job_title', 'bert_similarity', 'doc2vec_similarity', 'mean_score']].head(20))  # Display top sorted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df59cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
